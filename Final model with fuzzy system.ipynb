{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4acxEb9uvmk"
      },
      "source": [
        "# Jupyter code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5drfNyZEHoF"
      },
      "source": [
        "## Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sZ2gAafjD_js"
      },
      "outputs": [],
      "source": [
        "# Data Manipulation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# NLP for text pre-processing\n",
        "import nltk\n",
        "import scipy\n",
        "import re\n",
        "from scipy import spatial\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "tokenizer = ToktokTokenizer()\n",
        "\n",
        "# other libraries\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "import itertools\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.feature_extraction. text import TfidfVectorizer\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Import linear_kernel\n",
        "from sklearn.metrics.pairwise import linear_kernel\n",
        "\n",
        "# remove warnings\n",
        "import warnings\n",
        "warnings.filterwarnings (action = 'ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        },
        "id": "bZ2NcCrfELXU",
        "outputId": "bb09dfd3-3b3c-462e-9cab-aa60c1081224"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                            uniq_id            crawl_timestamp  \\\n",
              "0  c2d766ca982eca8304150849735ffef9  2016-03-25 22:59:23 +0000   \n",
              "1  7f7036a6d550aaa89d34c77bd39a5e48  2016-03-25 22:59:23 +0000   \n",
              "2  f449ec65dcbc041b6ae5e6a32717d01b  2016-03-25 22:59:23 +0000   \n",
              "3  0973b37acd0c664e3de26e97e5571454  2016-03-25 22:59:23 +0000   \n",
              "4  bc940ea42ee6bef5ac7cea3fb5cfbee7  2016-03-25 22:59:23 +0000   \n",
              "\n",
              "                                         product_url  \\\n",
              "0  http://www.flipkart.com/alisha-solid-women-s-c...   \n",
              "1  http://www.flipkart.com/fabhomedecor-fabric-do...   \n",
              "2  http://www.flipkart.com/aw-bellies/p/itmeh4grg...   \n",
              "3  http://www.flipkart.com/alisha-solid-women-s-c...   \n",
              "4  http://www.flipkart.com/sicons-all-purpose-arn...   \n",
              "\n",
              "                            product_name  \\\n",
              "0    Alisha Solid Women's Cycling Shorts   \n",
              "1    FabHomeDecor Fabric Double Sofa Bed   \n",
              "2                             AW Bellies   \n",
              "3    Alisha Solid Women's Cycling Shorts   \n",
              "4  Sicons All Purpose Arnica Dog Shampoo   \n",
              "\n",
              "                               product_category_tree               pid  \\\n",
              "0  [\"Clothing >> Women's Clothing >> Lingerie, Sl...  SRTEH2FF9KEDEFGF   \n",
              "1  [\"Furniture >> Living Room Furniture >> Sofa B...  SBEEH3QGU7MFYJFY   \n",
              "2  [\"Footwear >> Women's Footwear >> Ballerinas >...  SHOEH4GRSUBJGZXE   \n",
              "3  [\"Clothing >> Women's Clothing >> Lingerie, Sl...  SRTEH2F6HUZMQ6SJ   \n",
              "4  [\"Pet Supplies >> Grooming >> Skin & Coat Care...  PSOEH3ZYDMSYARJ5   \n",
              "\n",
              "   retail_price  discounted_price  \\\n",
              "0         999.0             379.0   \n",
              "1       32157.0           22646.0   \n",
              "2         999.0             499.0   \n",
              "3         699.0             267.0   \n",
              "4         220.0             210.0   \n",
              "\n",
              "                                               image is_FK_Advantage_product  \\\n",
              "0  [\"http://img5a.flixcart.com/image/short/u/4/a/...                   False   \n",
              "1  [\"http://img6a.flixcart.com/image/sofa-bed/j/f...                   False   \n",
              "2  [\"http://img5a.flixcart.com/image/shoe/7/z/z/r...                   False   \n",
              "3  [\"http://img5a.flixcart.com/image/short/6/2/h/...                   False   \n",
              "4  [\"http://img5a.flixcart.com/image/pet-shampoo/...                   False   \n",
              "\n",
              "                                         description       product_rating  \\\n",
              "0  Key Features of Alisha Solid Women's Cycling S...  No rating available   \n",
              "1  FabHomeDecor Fabric Double Sofa Bed (Finish Co...  No rating available   \n",
              "2  Key Features of AW Bellies Sandals Wedges Heel...  No rating available   \n",
              "3  Key Features of Alisha Solid Women's Cycling S...  No rating available   \n",
              "4  Specifications of Sicons All Purpose Arnica Do...  No rating available   \n",
              "\n",
              "        overall_rating         brand  \\\n",
              "0  No rating available        Alisha   \n",
              "1  No rating available  FabHomeDecor   \n",
              "2  No rating available            AW   \n",
              "3  No rating available        Alisha   \n",
              "4  No rating available        Sicons   \n",
              "\n",
              "                              product_specifications  \n",
              "0  {\"product_specification\"=>[{\"key\"=>\"Number of ...  \n",
              "1  {\"product_specification\"=>[{\"key\"=>\"Installati...  \n",
              "2  {\"product_specification\"=>[{\"key\"=>\"Ideal For\"...  \n",
              "3  {\"product_specification\"=>[{\"key\"=>\"Number of ...  \n",
              "4  {\"product_specification\"=>[{\"key\"=>\"Pet Type\",...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3f57b48a-1af7-4952-a039-76d20b2da75a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>uniq_id</th>\n",
              "      <th>crawl_timestamp</th>\n",
              "      <th>product_url</th>\n",
              "      <th>product_name</th>\n",
              "      <th>product_category_tree</th>\n",
              "      <th>pid</th>\n",
              "      <th>retail_price</th>\n",
              "      <th>discounted_price</th>\n",
              "      <th>image</th>\n",
              "      <th>is_FK_Advantage_product</th>\n",
              "      <th>description</th>\n",
              "      <th>product_rating</th>\n",
              "      <th>overall_rating</th>\n",
              "      <th>brand</th>\n",
              "      <th>product_specifications</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>c2d766ca982eca8304150849735ffef9</td>\n",
              "      <td>2016-03-25 22:59:23 +0000</td>\n",
              "      <td>http://www.flipkart.com/alisha-solid-women-s-c...</td>\n",
              "      <td>Alisha Solid Women's Cycling Shorts</td>\n",
              "      <td>[\"Clothing &gt;&gt; Women's Clothing &gt;&gt; Lingerie, Sl...</td>\n",
              "      <td>SRTEH2FF9KEDEFGF</td>\n",
              "      <td>999.0</td>\n",
              "      <td>379.0</td>\n",
              "      <td>[\"http://img5a.flixcart.com/image/short/u/4/a/...</td>\n",
              "      <td>False</td>\n",
              "      <td>Key Features of Alisha Solid Women's Cycling S...</td>\n",
              "      <td>No rating available</td>\n",
              "      <td>No rating available</td>\n",
              "      <td>Alisha</td>\n",
              "      <td>{\"product_specification\"=&gt;[{\"key\"=&gt;\"Number of ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>7f7036a6d550aaa89d34c77bd39a5e48</td>\n",
              "      <td>2016-03-25 22:59:23 +0000</td>\n",
              "      <td>http://www.flipkart.com/fabhomedecor-fabric-do...</td>\n",
              "      <td>FabHomeDecor Fabric Double Sofa Bed</td>\n",
              "      <td>[\"Furniture &gt;&gt; Living Room Furniture &gt;&gt; Sofa B...</td>\n",
              "      <td>SBEEH3QGU7MFYJFY</td>\n",
              "      <td>32157.0</td>\n",
              "      <td>22646.0</td>\n",
              "      <td>[\"http://img6a.flixcart.com/image/sofa-bed/j/f...</td>\n",
              "      <td>False</td>\n",
              "      <td>FabHomeDecor Fabric Double Sofa Bed (Finish Co...</td>\n",
              "      <td>No rating available</td>\n",
              "      <td>No rating available</td>\n",
              "      <td>FabHomeDecor</td>\n",
              "      <td>{\"product_specification\"=&gt;[{\"key\"=&gt;\"Installati...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>f449ec65dcbc041b6ae5e6a32717d01b</td>\n",
              "      <td>2016-03-25 22:59:23 +0000</td>\n",
              "      <td>http://www.flipkart.com/aw-bellies/p/itmeh4grg...</td>\n",
              "      <td>AW Bellies</td>\n",
              "      <td>[\"Footwear &gt;&gt; Women's Footwear &gt;&gt; Ballerinas &gt;...</td>\n",
              "      <td>SHOEH4GRSUBJGZXE</td>\n",
              "      <td>999.0</td>\n",
              "      <td>499.0</td>\n",
              "      <td>[\"http://img5a.flixcart.com/image/shoe/7/z/z/r...</td>\n",
              "      <td>False</td>\n",
              "      <td>Key Features of AW Bellies Sandals Wedges Heel...</td>\n",
              "      <td>No rating available</td>\n",
              "      <td>No rating available</td>\n",
              "      <td>AW</td>\n",
              "      <td>{\"product_specification\"=&gt;[{\"key\"=&gt;\"Ideal For\"...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0973b37acd0c664e3de26e97e5571454</td>\n",
              "      <td>2016-03-25 22:59:23 +0000</td>\n",
              "      <td>http://www.flipkart.com/alisha-solid-women-s-c...</td>\n",
              "      <td>Alisha Solid Women's Cycling Shorts</td>\n",
              "      <td>[\"Clothing &gt;&gt; Women's Clothing &gt;&gt; Lingerie, Sl...</td>\n",
              "      <td>SRTEH2F6HUZMQ6SJ</td>\n",
              "      <td>699.0</td>\n",
              "      <td>267.0</td>\n",
              "      <td>[\"http://img5a.flixcart.com/image/short/6/2/h/...</td>\n",
              "      <td>False</td>\n",
              "      <td>Key Features of Alisha Solid Women's Cycling S...</td>\n",
              "      <td>No rating available</td>\n",
              "      <td>No rating available</td>\n",
              "      <td>Alisha</td>\n",
              "      <td>{\"product_specification\"=&gt;[{\"key\"=&gt;\"Number of ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>bc940ea42ee6bef5ac7cea3fb5cfbee7</td>\n",
              "      <td>2016-03-25 22:59:23 +0000</td>\n",
              "      <td>http://www.flipkart.com/sicons-all-purpose-arn...</td>\n",
              "      <td>Sicons All Purpose Arnica Dog Shampoo</td>\n",
              "      <td>[\"Pet Supplies &gt;&gt; Grooming &gt;&gt; Skin &amp; Coat Care...</td>\n",
              "      <td>PSOEH3ZYDMSYARJ5</td>\n",
              "      <td>220.0</td>\n",
              "      <td>210.0</td>\n",
              "      <td>[\"http://img5a.flixcart.com/image/pet-shampoo/...</td>\n",
              "      <td>False</td>\n",
              "      <td>Specifications of Sicons All Purpose Arnica Do...</td>\n",
              "      <td>No rating available</td>\n",
              "      <td>No rating available</td>\n",
              "      <td>Sicons</td>\n",
              "      <td>{\"product_specification\"=&gt;[{\"key\"=&gt;\"Pet Type\",...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3f57b48a-1af7-4952-a039-76d20b2da75a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-3f57b48a-1af7-4952-a039-76d20b2da75a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-3f57b48a-1af7-4952-a039-76d20b2da75a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-f4c674fc-3d9b-423b-8864-c83de3cb5342\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f4c674fc-3d9b-423b-8864-c83de3cb5342')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-f4c674fc-3d9b-423b-8864-c83de3cb5342 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data",
              "summary": "{\n  \"name\": \"data\",\n  \"rows\": 20002,\n  \"fields\": [\n    {\n      \"column\": \"uniq_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 20000,\n        \"samples\": [\n          \"cbde249bb4d416b14712d6defac4ba6b\",\n          \"243f2b72bab00923359c75ec6528e3da\",\n          \"c69024ad5311db7c27d87e9c7ac14d28\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"crawl_timestamp\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 371,\n        \"samples\": [\n          \"2016-03-19 07:04:11 +0000\",\n          \"2016-06-17 11:45:06 +0000\",\n          \"2016-06-12 08:33:38 +0000\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"product_url\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 20000,\n        \"samples\": [\n          \"http://www.flipkart.com/avaron-projekt-moustache-brooch/p/itmegap67zyufvyq?pid=BCHEGAP6ZKD7ZSR7\",\n          \"http://www.flipkart.com/grafion-comfort-feel-women-s-tube-bra/p/itmebcy3q2sy9fsy?pid=BRAEBCY3MM4KMB3Q\",\n          \"http://www.flipkart.com/blessed-ring-plant-container-set/p/itmdy5axh37gtgj3?pid=PCSDY5AHUNGBYFXH\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"product_name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 12676,\n        \"samples\": [\n          \"I Am For You Casual Full Sleeve Solid Women's Top\",\n          \"Clovia Lingerie Set\",\n          \"Cotonex Blue, Pink Cotton Kitchen Linen Set\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"product_category_tree\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 6466,\n        \"samples\": [\n          \"[\\\"Home Decor & Festive Needs >> Table Decor & Handicrafts >> Showpieces >> Religious Idols >> Divinit Religious Idols\\\"]\",\n          \"[\\\"Mobiles & Accessories >> Mobile Accessories >> Mobile Pouches >> kits kart Mobile Pouches >> kits kart Pouch for HTC One M9+ Supreme Camera (...\\\"]\",\n          \"[\\\"Clothing >> Women's Clothing >> Western Wear >> Shirts, Tops & Tunics >> Polos & T-Shirts >> Go-Art Polos & T-Shirts\\\"]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pid\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 19998,\n        \"samples\": [\n          \"SWSEBWYCGGNCRXJH\",\n          \"NKCDZHF7GVHHGZRJ\",\n          \"SHOE6XNZ7WS4ZQXH\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"retail_price\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 9009.639341426717,\n        \"min\": 35.0,\n        \"max\": 571230.0,\n        \"num_unique_values\": 2247,\n        \"samples\": [\n          36543.0,\n          25730.0,\n          6000.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"discounted_price\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 7333.586040242493,\n        \"min\": 35.0,\n        \"max\": 571230.0,\n        \"num_unique_values\": 2448,\n        \"samples\": [\n          291.0,\n          1485.0,\n          411.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"image\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 18589,\n        \"samples\": [\n          \"[\\\"http://img5a.flixcart.com/image/pet-apparel/e/v/g/super-water-absorbing-durable-microfiber-dog-pet-towel-four-paws-1100x1100-imaeery4yh7zmbn4.jpeg\\\", \\\"http://img6a.flixcart.com/image/pet-apparel/e/v/g/super-water-absorbing-durable-microfiber-dog-pet-towel-four-paws-original-imaeery4yh7zmbn4.jpeg\\\", \\\"http://img5a.flixcart.com/image/pet-apparel/e/v/g/super-water-absorbing-durable-microfiber-dog-pet-towel-four-paws-original-imaeery4gyggxjw6.jpeg\\\"]\",\n          \"[\\\"http://img5a.flixcart.com/image/legging-jegging/g/s/z/1-1-sg16-sg03118-beauty-fits-free-original-imae8p2ktvybzdcm.jpeg\\\", \\\"http://img6a.flixcart.com/image/legging-jegging/g/s/z/1-1-sg16-sg03118-beauty-fits-free-original-imae8p2ktvybzdcm.jpeg\\\", \\\"http://img5a.flixcart.com/image/legging-jegging/y/v/a/1-1-pp04-pp18-pp16-kimmy-free-original-imae8p2evn7pedmq.jpeg\\\", \\\"http://img6a.flixcart.com/image/legging-jegging/b/v/y/1-1-pp04-pp03-pp07-kimmy-free-original-imae8p2eqrfq6pyr.jpeg\\\"]\",\n          \"[\\\"http://img5a.flixcart.com/image/bra/4/c/k/tube-pink1-luxemburg-38-original-imae2k5qqfhgbsjj.jpeg\\\", \\\"http://img6a.flixcart.com/image/bra/4/c/k/tube-pink1-luxemburg-40-original-imae2hagrxg4dycr.jpeg\\\"]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"is_FK_Advantage_product\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          true,\n          false\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"description\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 17539,\n        \"samples\": [\n          \"ShowTime Women's Full Coverage, T-Shirt Bra - Buy White ShowTime Women's Full Coverage, T-Shirt Bra For Only Rs. 450 Online in India. Shop Online For Apparels. Huge Collection of Branded Clothes Only at Flipkart.com\",\n          \"Remanika Women's Dress - Buy Black, White Remanika Women's Dress For Only Rs. 1600 Online in India. Shop Online For Apparels. Huge Collection of Branded Clothes Only at Flipkart.com\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"product_rating\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 36,\n        \"samples\": [\n          \"1.8\",\n          \"3.2\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"overall_rating\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 36,\n        \"samples\": [\n          \"1.8\",\n          \"3.2\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"brand\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3499,\n        \"samples\": [\n          \"Wonderland\",\n          \"Radhika's World of Crafts\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"product_specifications\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 18825,\n        \"samples\": [\n          \"{\\\"product_specification\\\"=>[{\\\"key\\\"=>\\\"Brand\\\", \\\"value\\\"=>\\\"Allure Auto\\\"}, {\\\"key\\\"=>\\\"Model Year\\\", \\\"value\\\"=>\\\"NA\\\"}, {\\\"key\\\"=>\\\"Vehicle Model Name\\\", \\\"value\\\"=>\\\"WagonR\\\"}, {\\\"key\\\"=>\\\"Model Number\\\", \\\"value\\\"=>\\\"Maruti Wagonr - Rubber Mats ( Smoke Transparent)\\\"}, {\\\"key\\\"=>\\\"Vehicle Brand\\\", \\\"value\\\"=>\\\"Maruti\\\"}, {\\\"key\\\"=>\\\"Type\\\", \\\"value\\\"=>\\\"Floor Mat\\\"}, {\\\"key\\\"=>\\\"Material\\\", \\\"value\\\"=>\\\"Rubber\\\"}, {\\\"key\\\"=>\\\"Model Name\\\", \\\"value\\\"=>\\\"CM 941\\\"}, {\\\"key\\\"=>\\\"Color\\\", \\\"value\\\"=>\\\"Black\\\"}, {\\\"key\\\"=>\\\"Sales Package\\\", \\\"value\\\"=>\\\"Set of 4 Car Floor Mats\\\"}, {\\\"key\\\"=>\\\"Pack of\\\", \\\"value\\\"=>\\\"4\\\"}]}\",\n          \"{\\\"product_specification\\\"=>[{\\\"key\\\"=>\\\"Wireless Speed\\\", \\\"value\\\"=>\\\"300 mbps\\\"}, {\\\"key\\\"=>\\\"Brand\\\", \\\"value\\\"=>\\\"TRENDnet\\\"}, {\\\"key\\\"=>\\\"In The Box\\\", \\\"value\\\"=>\\\"CD-ROM (Users Guide), THA-101, Multi-Language Quick Installation Guide\\\"}, {\\\"key\\\"=>\\\"Model\\\", \\\"value\\\"=>\\\"THA-101 N300 Router\\\"}, {\\\"key\\\"=>\\\"Type\\\", \\\"value\\\"=>\\\"Range Extenders/Repeaters\\\"}, {\\\"key\\\"=>\\\"Color\\\", \\\"value\\\"=>\\\"White\\\"}, {\\\"key\\\"=>\\\"Warranty Summary\\\", \\\"value\\\"=>\\\"3 Year Manufacturer Warranty\\\"}, {\\\"key\\\"=>\\\"Number of USB Ports\\\", \\\"value\\\"=>\\\"0\\\"}, {\\\"key\\\"=>\\\"Antennae\\\", \\\"value\\\"=>\\\"External\\\"}]}\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "data = pd.read_csv(\"/content/flipkart_com-ecommerce_sample.csv\") # Changed the file path\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NoXu_bBFFX-c"
      },
      "source": [
        "## Data Exploration - jupyter file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TexccyB4Fddt"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mUWRBiB4EaTt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 554
        },
        "outputId": "575d9dda-6972-4577-d9c3-768836988bd2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "uniq_id                       2\n",
              "crawl_timestamp               2\n",
              "product_url                   2\n",
              "product_name                  2\n",
              "product_category_tree         2\n",
              "pid                           2\n",
              "retail_price                 80\n",
              "discounted_price             80\n",
              "image                         5\n",
              "is_FK_Advantage_product       2\n",
              "description                   4\n",
              "product_rating                2\n",
              "overall_rating                2\n",
              "brand                      5866\n",
              "product_specifications       16\n",
              "dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>uniq_id</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>crawl_timestamp</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>product_url</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>product_name</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>product_category_tree</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pid</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>retail_price</th>\n",
              "      <td>80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>discounted_price</th>\n",
              "      <td>80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>image</th>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>is_FK_Advantage_product</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>description</th>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>product_rating</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>overall_rating</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>brand</th>\n",
              "      <td>5866</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>product_specifications</th>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "data.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1yxyCZ-FgUa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 519
        },
        "outputId": "bb9028a7-6908-46d3-b5bc-5149746fc10a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                         missing    percent\n",
              "brand                       5866  29.327067\n",
              "discounted_price              80   0.399960\n",
              "retail_price                  80   0.399960\n",
              "product_specifications        16   0.079992\n",
              "image                          5   0.024998\n",
              "description                    4   0.019998\n",
              "crawl_timestamp                2   0.009999\n",
              "pid                            2   0.009999\n",
              "product_category_tree          2   0.009999\n",
              "product_name                   2   0.009999\n",
              "product_url                    2   0.009999\n",
              "uniq_id                        2   0.009999\n",
              "is_FK_Advantage_product        2   0.009999\n",
              "overall_rating                 2   0.009999\n",
              "product_rating                 2   0.009999"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-908dbcf8-e095-4f64-b0d5-996e7c676aa5\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>missing</th>\n",
              "      <th>percent</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>brand</th>\n",
              "      <td>5866</td>\n",
              "      <td>29.327067</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>discounted_price</th>\n",
              "      <td>80</td>\n",
              "      <td>0.399960</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>retail_price</th>\n",
              "      <td>80</td>\n",
              "      <td>0.399960</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>product_specifications</th>\n",
              "      <td>16</td>\n",
              "      <td>0.079992</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>image</th>\n",
              "      <td>5</td>\n",
              "      <td>0.024998</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>description</th>\n",
              "      <td>4</td>\n",
              "      <td>0.019998</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>crawl_timestamp</th>\n",
              "      <td>2</td>\n",
              "      <td>0.009999</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pid</th>\n",
              "      <td>2</td>\n",
              "      <td>0.009999</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>product_category_tree</th>\n",
              "      <td>2</td>\n",
              "      <td>0.009999</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>product_name</th>\n",
              "      <td>2</td>\n",
              "      <td>0.009999</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>product_url</th>\n",
              "      <td>2</td>\n",
              "      <td>0.009999</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>uniq_id</th>\n",
              "      <td>2</td>\n",
              "      <td>0.009999</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>is_FK_Advantage_product</th>\n",
              "      <td>2</td>\n",
              "      <td>0.009999</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>overall_rating</th>\n",
              "      <td>2</td>\n",
              "      <td>0.009999</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>product_rating</th>\n",
              "      <td>2</td>\n",
              "      <td>0.009999</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-908dbcf8-e095-4f64-b0d5-996e7c676aa5')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-908dbcf8-e095-4f64-b0d5-996e7c676aa5 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-908dbcf8-e095-4f64-b0d5-996e7c676aa5');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-3565ff44-fd29-476f-bd77-e32a552aa364\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-3565ff44-fd29-476f-bd77-e32a552aa364')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-3565ff44-fd29-476f-bd77-e32a552aa364 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"missing\",\n  \"rows\": 15,\n  \"fields\": [\n    {\n      \"column\": \"missing\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1511,\n        \"min\": 2,\n        \"max\": 5866,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          5866,\n          80,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"percent\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 7.554702774310671,\n        \"min\": 0.009999000099990002,\n        \"max\": 29.327067293270677,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          29.327067293270677,\n          0.3999600039996,\n          0.009999000099990002\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "#handling missing values\n",
        "missing = pd.DataFrame(data.isnull().sum()).rename (columns = {0: 'missing' })\n",
        "missing['percent'] = (missing['missing'] /len(data))*100\n",
        "missing.sort_values ('percent', ascending = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2HJB5WMxFjN6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c55b674-64c2-4f68-b4a1-92180f4d8121"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of duplicate rows: 1\n",
            "Duplicate rows found:\n",
            "      uniq_id crawl_timestamp product_url product_name product_category_tree  \\\n",
            "20001     NaN             NaN         NaN          NaN                   NaN   \n",
            "\n",
            "       pid  retail_price  discounted_price image is_FK_Advantage_product  \\\n",
            "20001  NaN           NaN               NaN   NaN                     NaN   \n",
            "\n",
            "      description product_rating overall_rating brand product_specifications  \n",
            "20001         NaN            NaN            NaN   NaN                    NaN  \n",
            "Original dataset shape: (20002, 15)\n",
            "Dataset shape after removing duplicates: (20001, 15)\n"
          ]
        }
      ],
      "source": [
        "# fing ing the redundant or duplicate rows and removingthem\n",
        "\n",
        "duplicate_rows = data[data.duplicated()]\n",
        "\n",
        "# Print the number of duplicate rows found\n",
        "print(f\"Number of duplicate rows: {duplicate_rows.shape[0]}\")\n",
        "\n",
        "# Display the duplicate rows (if any)\n",
        "if duplicate_rows.shape[0] > 0:\n",
        "    print(\"Duplicate rows found:\")\n",
        "    print(duplicate_rows)\n",
        "else:\n",
        "    print(\"No duplicate rows found.\")\n",
        "\n",
        "# Remove duplicate rows and keep the first occurrence\n",
        "data_cleaned = data.drop_duplicates()\n",
        "\n",
        "# Verify the result by checking the new shape of the dataset\n",
        "print(f\"Original dataset shape: {data.shape}\")\n",
        "print(f\"Dataset shape after removing duplicates: {data_cleaned.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yH4Aw58Fso-"
      },
      "source": [
        "### Text preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSp8IyQmFwNq"
      },
      "source": [
        "There is a lot of unwanted information present in the text data. Let's clean it up. Text preprocessing tasks include\n",
        "\n",
        "* Converting the text data to lowercase\n",
        "* Removing/replacing the punctuations\n",
        "* Removing/replacing the numbers\n",
        "* Removing extra whitespaces\n",
        "* Removing stop words\n",
        "* Stemming and lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_gHl9ibsFm4U",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "eb741039-c2c1-43a2-c925-7677075e1e6a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    key features of alisha solid women's cycling s...\n",
              "1    fabhomedecor fabric double sofa bed (finish co...\n",
              "2    key features of aw bellies sandals wedges heel...\n",
              "3    key features of alisha solid women's cycling s...\n",
              "4    specifications of sicons all purpose arnica do...\n",
              "Name: description, dtype: object"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>description</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>key features of alisha solid women's cycling s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>fabhomedecor fabric double sofa bed (finish co...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>key features of aw bellies sandals wedges heel...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>key features of alisha solid women's cycling s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>specifications of sicons all purpose arnica do...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> object</label>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "#to lowercase\n",
        "data['description'] = data['description'].str.lower()\n",
        "\n",
        "#removing punctivations\n",
        "data['description'] = data['description'].str.replace(r'[^\\w\\d\\s]',' ')\n",
        "\n",
        "#replacing whitespace between terms with a single space\n",
        "data['description'] = data['description'].str.replace(r'\\s+',' ')\n",
        "\n",
        "#removing leading and trailing whitespace\n",
        "data['description'] = data['description'].str.replace(r'^\\s+|\\s+?$','')\n",
        "\n",
        "data['description'].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UnZLH-MXF0RQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c23c8e7a-b425-4585-898e-c939fdb49008"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "#Removing stop words\n",
        "stop=stopwords.words('english')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KK2ivo7UF3DK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "71bb2aa9-7471-40e7-cd0e-4fd46427c179"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    features of alisha solid women's cycling short...\n",
              "1    fabhomedecor fabric double sofa bed (finish co...\n",
              "2    features of aw bellies sandals wedges heel cas...\n",
              "3    features of alisha solid women's cycling short...\n",
              "4    specifications of sicons all purpose arnica do...\n",
              "Name: description, dtype: object"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>description</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>features of alisha solid women's cycling short...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>fabhomedecor fabric double sofa bed (finish co...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>features of aw bellies sandals wedges heel cas...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>features of alisha solid women's cycling short...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>specifications of sicons all purpose arnica do...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> object</label>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "pattern = r'\\b(?:{})\\b'.format('|'.join(stop))\n",
        "data['description'] = data['description'].str.replace(pattern, '')\n",
        "\n",
        "# Removing single characters\n",
        "data['description'] = data['description'].str.replace(r'\\s+', ' ')\n",
        "data['description'] = data['description'].apply(lambda x: \" \".join([word for word in str(x).split() if len(word) > 1]))\n",
        "\n",
        "# Removing domain related stop words from description\n",
        "specific_stop_words = [\"rs\", \"flipkart\", \"buy\", \"com\", \"free\", \"day\", \"cash\", \"replacement\", \"guarantee\", \"genuine\", \"key\", \"feature\", \"delivery\", \"products\", \"product\", \"shipping\", \"online\", \"india\", \"shop\"]\n",
        "data['description'] = data['description'].apply(lambda x: \" \".join(word for word in str(x).split() if word not in specific_stop_words))\n",
        "\n",
        "data['description'].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Bbq33EGGACa"
      },
      "source": [
        "### Visualizing the most occured words in corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VnHAkZblF6FT",
        "outputId": "fcb4e7a7-1f84-486e-b975-96c9812285a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "# Download the required NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 754
        },
        "id": "ijtxsMVEGDDK",
        "outputId": "61c35712-ccdf-4c93-a1af-b34e7c7a8e6a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-d2555157039d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Tokenize the text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Filter out non-alphabetic words and stopwords (both generic and domain-specific)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \"\"\"\n\u001b[0;32m--> 142\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m     return [\n\u001b[1;32m    144\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \"\"\"\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_punkt_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36m_get_punkt_tokenizer\u001b[0;34m(language)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \"\"\"\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mPunktTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1742\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1743\u001b[0m         \u001b[0mPunktSentenceTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1744\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_lang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_lang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mload_lang\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1747\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1749\u001b[0;31m         \u001b[0mlang_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"tokenizers/punkt_tab/{lang}/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1750\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_punkt_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1751\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
          ]
        }
      ],
      "source": [
        "#most frequent words after removing domain related stopwords\n",
        "\n",
        "# Custom stopwords list (including 'rs' and other domain-specific terms)\n",
        "custom_stopwords = stopwords.words('english') + ['rs', 'type','details','guarantee','product', 'products', 'delivery', 'shipping', 'cm','price', 'features']\n",
        "\n",
        "# Concatenate all product descriptions into a single string\n",
        "a = data['description'].str.cat(sep=' ')\n",
        "\n",
        "# Tokenize the text\n",
        "words = nltk.tokenize.word_tokenize(a)\n",
        "\n",
        "# Filter out non-alphabetic words and stopwords (both generic and domain-specific)\n",
        "words = [word for word in words if re.match(r'^[a-zA-Z]+$', word) and word.lower() not in custom_stopwords]\n",
        "\n",
        "# Create a frequency distribution of the remaining words\n",
        "word_dist = nltk.FreqDist(words)\n",
        "\n",
        "# Plot the top 10 most frequent words\n",
        "plt.figure(figsize=(10, 6))\n",
        "word_dist.plot(10, cumulative=False)\n",
        "\n",
        "# Print the top 10 most frequent words\n",
        "print(word_dist.most_common(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XjCerHk_3dcV"
      },
      "outputs": [],
      "source": [
        "print(word_dist.most_common(15))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GPXDOkZ-GIN-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLB47el1GVjW"
      },
      "source": [
        "# Advanced Search Engine Using PyTerrier and Sentence-BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBTKHXlEnzOz"
      },
      "source": [
        "#Title of the Advanced Search Engine - \"Hybrid Semantic Search Engine Using PyTerrier, SymSpell, and Sentence-BERT for Enhanced Product Retrieval\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zS0URn5c4zNM"
      },
      "source": [
        "#### installing required libraries and making setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gOnPYU0r4t_r"
      },
      "outputs": [],
      "source": [
        "# Install necessary packages\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "#developed by facebook ai reasearch lab, provides tools for building and training neural networks\n",
        "#url is to download the CUDA version for GPU acceleration\n",
        "!pip install -U sentence-transformers\n",
        "#built on top of hugging face transofrmers\n",
        "\"\"\"It simplifies the process of generating dense vector representations (embeddings) of sentences,\n",
        "paragraphs, or documents. These embeddings can be used for tasks like semantic search, text similarity, clustering, and more.\n",
        "used in models like BERT , RoBERTa, and DistilBERT. to work on NLP models\"\"\"\n",
        "\n",
        "!pip install python-terrier\n",
        "#It provides tools for indexing, querying, and evaluating retrieval systems\n",
        "!pip install nltk\n",
        "#Natural Language Toolkit (NLTK), ibraries for tokenization, stemming, lemmatization, part-of-speech tagging, parsing, and more.\n",
        "!pip install scikit-learn\n",
        "#simple and efficient tool for data mining and data analysis, built on NumPy, SciPy, and matplotlib\n",
        "!pip install symspellpy\n",
        "#an efficient spelling correction algorithm.\n",
        "\n",
        "# Import libraries\n",
        "import pandas as pd# data manipulation and analysis\n",
        "import numpy as np #numerical computing in Python.\n",
        "import string #contains a collection of string constants (e.g., punctuation characters, digits, letters).\n",
        "import re #module provides support for regular expressions.\n",
        "from nltk.corpus import stopwords #list of common words (e.g., \"the\", \"and\", \"is\") that are often removed from text because they don't contribute much to the meaning.\n",
        "from nltk.tokenize import word_tokenize # splits text into individual words or tokens.\n",
        "from nltk.stem.wordnet import WordNetLemmatizer #reduces words to their base or dictionary form (lemma).\n",
        "import nltk\n",
        "nltk.download('punkt')# for tokenization\n",
        "nltk.download('wordnet') #for lemmatization\n",
        "nltk.download('stopwords') #for stopword removal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQY9Tf048GHD"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade python-terrier\n",
        "# Initialize PyTerrier -  for information retrieval (IR) tasks.\n",
        "import pyterrier as pt\n",
        "# Check if PyTerrier is already initialized. If not, initialize it\n",
        "if not pt.started():  # Replace pt.started() with pt.init()\n",
        "    pt.init()  # This line initializes PyTerrier\n",
        "\n",
        "# Uninstall the current torchvision\n",
        "!pip uninstall -y torchvision\n",
        "\n",
        "# Reinstall torchvision specifying the CUDA version\n",
        "!pip install torchvision --index-url https://download.pytorch.org/whl/cu118\n",
        "\n",
        "# Import SentenceTransformer for embeddings\n",
        "from sentence_transformers import SentenceTransformer\n",
        "model = SentenceTransformer('sentence-transformers/bert-base-nli-mean-tokens')\n",
        "\n",
        "# Import SymSpell for spelling correction\n",
        "from symspellpy import SymSpell, Verbosity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4t-oawH5A_l"
      },
      "source": [
        "#### Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j1OYtbTE5In-"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv(\"/content/flipkart_com-ecommerce_sample.csv\")\n",
        "\n",
        "# Clean product category tree\n",
        "df['product_category_tree'] = df['product_category_tree'].str.replace('>>', ',')\n",
        "df['product_category_tree'] = df['product_category_tree'].str.replace('\"', '')\n",
        "\n",
        "# Drop unnecessary columns\n",
        "df.drop(['product_url', 'image', \"retail_price\", \"discounted_price\",\n",
        "         \"is_FK_Advantage_product\", \"product_rating\", \"overall_rating\", \"product_specifications\"],\n",
        "        axis=1, inplace=True)\n",
        "\n",
        "# Remove duplicate products\n",
        "uniq_prod = df.copy()\n",
        "uniq_prod.drop_duplicates(subset=\"product_name\", keep=\"first\", inplace=True)\n",
        "\n",
        "# Define stopwords and punctuation\n",
        "stop_words = set(stopwords.words('english'))\n",
        "exclude = set(string.punctuation)\n",
        "lem = WordNetLemmatizer()\n",
        "\n",
        "# Function to clean text\n",
        "def filter_keywords(doc):\n",
        "    doc = doc.lower()\n",
        "    stop_free = \" \".join([i for i in doc.split() if i not in stop_words])\n",
        "    punc_free = \"\".join(ch for ch in stop_free if ch not in exclude)\n",
        "    word_tokens = word_tokenize(punc_free)  # Tokenize the text\n",
        "    filtered_sentence = [(lem.lemmatize(w, \"v\")) for w in word_tokens]  # Lemmatize tokens\n",
        "    return \" \".join(filtered_sentence)\n",
        "\n",
        "# Apply cleaning to relevant columns\n",
        "# Convert the 'product_name' column to string before applying filter_keywords\n",
        "uniq_prod['product'] = uniq_prod['product_name'].astype(str).apply(filter_keywords)\n",
        "uniq_prod['brand'] = uniq_prod['brand'].astype(str).apply(filter_keywords)\n",
        "uniq_prod['description'] = uniq_prod['description'].astype(str).apply(filter_keywords)\n",
        "\n",
        "# Combine all keywords for each product\n",
        "uniq_prod[\"keywords\"] = (\n",
        "    uniq_prod['product'] + \" \" +\n",
        "    uniq_prod['brand'] + \" \" +\n",
        "    uniq_prod['product_category_tree']\n",
        ")\n",
        "\n",
        "# Create a 'docno' column for recommendations\n",
        "uniq_prod['docno'] = uniq_prod['product_name'].astype(str)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuJR2d_J5QPM"
      },
      "source": [
        "#### Spell Correction with SymSpell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sMZEP-WlDBVh"
      },
      "outputs": [],
      "source": [
        "# Download dictionaries for SymSpell\n",
        "!wget https://raw.githubusercontent.com/mammothb/symspellpy/master/symspellpy/frequency_dictionary_en_82_765.txt\n",
        "!wget https://raw.githubusercontent.com/mammothb/symspellpy/master/symspellpy/frequency_bigramdictionary_en_243_342.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ABzTFMpK5Tz1"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Initialize SymSpell\n",
        "sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
        "\n",
        "# Load the pre-built dictionary\n",
        "dictionary_path = \"frequency_dictionary_en_82_765.txt\"\n",
        "bigram_path = \"frequency_bigramdictionary_en_243_342.txt\"\n",
        "\n",
        "if not sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1):\n",
        "    print(\"Dictionary file not found!\")\n",
        "if not sym_spell.load_bigram_dictionary(bigram_path, term_index=0, count_index=2):\n",
        "    print(\"Bigram dictionary file not found!\")\n",
        "\n",
        "# Function to correct spelling\n",
        "def correct_spelling(text):\n",
        "    suggestions = sym_spell.lookup_compound(text, max_edit_distance=2)\n",
        "    corrected_text = suggestions[0].term\n",
        "    return corrected_text\n",
        "\n",
        "# Apply spell correction to the keywords\n",
        "uniq_prod[\"corrected_keywords\"] = uniq_prod[\"keywords\"].astype(str).apply(correct_spelling)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHS_9oYb6ttB"
      },
      "source": [
        "####  Indexing with PyTerrier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fUe_VRgZ6wv0"
      },
      "outputs": [],
      "source": [
        "# Create a DataFrame for indexing\n",
        "index_data = uniq_prod[['docno', 'corrected_keywords']]\n",
        "index_data.columns = ['docno', 'text']\n",
        "\n",
        "# Index the data\n",
        "indexer = pt.DFIndexer(\"./index\", overwrite=True)# creates an instance of a document frequency (DF) indexer\n",
        "index_ref = indexer.index(index_data['text'], index_data['docno']) # This line indexes the documents, a reference to the created index.\n",
        "\n",
        "# Retrieve documents using BatchRetrieve\n",
        "retriever = pt.BatchRetrieve(index_ref, wmodel=\"BM25\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lM6ffp96zw9"
      },
      "source": [
        "#### Semantic Search with Sentence-BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4_Gthehl61_d"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Function to compute semantic similarity\n",
        "def compute_semantic_similarity(query, documents):\n",
        "    query_embedding = model.encode([query])\n",
        "    document_embeddings = model.encode(documents)\n",
        "    similarities = cosine_similarity(query_embedding, document_embeddings).flatten()\n",
        "    return similarities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWTBLdzd65yE"
      },
      "source": [
        "#### Combined Search and Ranking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aJebyDWK673f"
      },
      "outputs": [],
      "source": [
        "# Function to take user input and display results\n",
        "def search_products(query):\n",
        "    # Step 1: Spell correction\n",
        "    corrected_query = correct_spelling(query)\n",
        "    print(f\"Corrected Query: {corrected_query}\")\n",
        "\n",
        "    # Step 2: Retrieve documents using BM25\n",
        "    results = retriever.search(corrected_query)\n",
        "    results = results.merge(uniq_prod, on='docno', how='left')\n",
        "\n",
        "    # Step 3: Compute semantic similarity\n",
        "    similarities = compute_semantic_similarity(corrected_query, results['corrected_keywords'].tolist())\n",
        "    results['similarity_score'] = similarities\n",
        "\n",
        "    # Step 4: Rank results by combining BM25 and semantic similarity\n",
        "    results['final_score'] = results['score'] + results['similarity_score']\n",
        "    ranked_results = results.sort_values(by='final_score', ascending=False)\n",
        "\n",
        "    # Display top results\n",
        "    return ranked_results[['product_name', 'brand', 'description', 'final_score']].head(10)\n",
        "\n",
        "# Take user input\n",
        "user_query = input(\"Enter your search query: \")\n",
        "search_results = search_products(user_query)\n",
        "print(\"\\nSearch Results:\")\n",
        "search_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jn0JjSnx4s3z"
      },
      "source": [
        "************"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y83zb5Lwqkaw"
      },
      "source": [
        "# Final code including soft computng Implementation -Fuzzy logic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyZT83JRGAMT"
      },
      "source": [
        "*************"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TR22uNStK5k6"
      },
      "outputs": [],
      "source": [
        "# Install necessary packages\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install -U sentence-transformers\n",
        "!pip install python-terrier\n",
        "!pip install nltk\n",
        "!pip install scikit-learn\n",
        "!pip install symspellpy\n",
        "!pip install fuzzywuzzy\n",
        "!pip install python-Levenshtein\n",
        "!pip install spacy\n",
        "\n",
        "# Download spaCy model\n",
        "!python -m spacy download en_core_web_md"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fSfArJVULNKM"
      },
      "outputs": [],
      "source": [
        "# # Uninstall the current torchvision\n",
        "# !pip uninstall -y torchvision\n",
        "\n",
        "# # Reinstall torchvision specifying the CUDA version\n",
        "# !pip install torchvision --index-url https://download.pytorch.org/whl/cu118"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjzRXXtMGxuQ"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Initialize PyTerrier\n",
        "import pyterrier as pt\n",
        "if not pt.started():\n",
        "    pt.init()\n",
        "\n",
        "# Import SentenceTransformer for embeddings\n",
        "from sentence_transformers import SentenceTransformer\n",
        "model = SentenceTransformer('sentence-transformers/bert-base-nli-mean-tokens')\n",
        "\n",
        "# Import SymSpell for spelling correction\n",
        "from symspellpy import SymSpell, Verbosity\n",
        "\n",
        "# Import FuzzyWuzzy for fuzzy matching\n",
        "from fuzzywuzzy import fuzz\n",
        "\n",
        "# Import spaCy for query expansion\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_md\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uTETUzDLNse2"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3SAlHxbZG2FB"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "\n",
        "df = pd.read_csv(\"/content/flipkart_com-ecommerce_sample.csv\")\n",
        "\n",
        "# Clean product category tree\n",
        "df['product_category_tree'] = df['product_category_tree'].str.replace('>>', ',')\n",
        "df['product_category_tree'] = df['product_category_tree'].str.replace('\"', '')\n",
        "\n",
        "# Drop unnecessary columns\n",
        "df.drop(['product_url', 'image', \"retail_price\", \"discounted_price\",\n",
        "         \"is_FK_Advantage_product\", \"product_rating\", \"overall_rating\", \"product_specifications\"],\n",
        "        axis=1, inplace=True)\n",
        "\n",
        "# Remove duplicate products\n",
        "uniq_prod = df.copy()\n",
        "uniq_prod.drop_duplicates(subset=\"product_name\", keep=\"first\", inplace=True)\n",
        "\n",
        "# Define stopwords and punctuation\n",
        "stop_words = set(stopwords.words('english'))\n",
        "exclude = set(string.punctuation)\n",
        "lem = WordNetLemmatizer()\n",
        "\n",
        "# Function to clean text\n",
        "def filter_keywords(doc):\n",
        "    doc = doc.lower()\n",
        "    stop_free = \" \".join([i for i in doc.split() if i not in stop_words])\n",
        "    punc_free = \"\".join(ch for ch in stop_free if ch not in exclude)\n",
        "    word_tokens = word_tokenize(punc_free)  # Tokenize the text\n",
        "    filtered_sentence = [(lem.lemmatize(w, \"v\")) for w in word_tokens]  # Lemmatize tokens\n",
        "    return \" \".join(filtered_sentence)\n",
        "\n",
        "# Apply cleaning to relevant columns\n",
        "uniq_prod['product'] = uniq_prod['product_name'].astype(str).apply(filter_keywords)\n",
        "uniq_prod['brand'] = uniq_prod['brand'].astype(\"str\").apply(filter_keywords)\n",
        "uniq_prod['description'] = uniq_prod['description'].astype(str).apply(filter_keywords)\n",
        "\n",
        "# Combine all keywords for each product\n",
        "uniq_prod[\"keywords\"] = (\n",
        "    uniq_prod['product'] + \" \" +\n",
        "    uniq_prod['brand'] + \" \" +\n",
        "    uniq_prod['product_category_tree']\n",
        ")\n",
        "\n",
        "# Create a 'docno' column for recommendations\n",
        "uniq_prod['docno'] = uniq_prod['product_name'].astype(str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8jsYQAJ8OsV4"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N7itGW4MGseh"
      },
      "outputs": [],
      "source": [
        "# Function to expand query with related words using spaCy\n",
        "def expand_query(query, topn=5):\n",
        "    expanded_words = []\n",
        "    doc = nlp(query)\n",
        "\n",
        "    for token in doc:\n",
        "        # Prioritize nouns over adjectives and ensure the token has a valid vector\n",
        "        if token.pos_ == \"NOUN\" and token.has_vector:\n",
        "            try:\n",
        "                # Find the most similar words to the current noun\n",
        "                similar_words = [\n",
        "                    word.text for word in nlp.vocab\n",
        "                    if word.has_vector and nlp(token.text).similarity(nlp(word.text)) > 0.7\n",
        "                ][:topn]\n",
        "                expanded_words.extend(similar_words)\n",
        "            except KeyError:\n",
        "                continue\n",
        "\n",
        "    # Remove duplicates and filter out irrelevant words\n",
        "    expanded_words = list(set(expanded_words))\n",
        "    return \" \".join(expanded_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M4fUzhTMD-K-"
      },
      "outputs": [],
      "source": [
        "# # Download dictionaries for SymSpell\n",
        "# !wget https://raw.githubusercontent.com/mammothb/symspellpy/master/symspellpy/frequency_dictionary_en_82_765.txt\n",
        "# !wget https://raw.githubusercontent.com/mammothb/symspellpy/master/symspellpy/frequency_bigramdictionary_en_243_342.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hwyFsC9DHE9A"
      },
      "outputs": [],
      "source": [
        "# Initialize SymSpell\n",
        "sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
        "\n",
        "# Load the pre-built dictionary\n",
        "dictionary_path = \"frequency_dictionary_en_82_765.txt\"\n",
        "bigram_path = \"frequency_bigramdictionary_en_243_342.txt\"\n",
        "\n",
        "if not sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1):\n",
        "    print(\"Dictionary file not found!\")\n",
        "if not sym_spell.load_bigram_dictionary(bigram_path, term_index=0, count_index=2):\n",
        "    print(\"Bigram dictionary file not found!\")\n",
        "\n",
        "# Function to correct spelling\n",
        "def correct_spelling(text):\n",
        "    suggestions = sym_spell.lookup_compound(text, max_edit_distance=2)\n",
        "    corrected_text = suggestions[0].term\n",
        "    return corrected_text\n",
        "\n",
        "# Apply spell correction to the keywords\n",
        "uniq_prod[\"corrected_keywords\"] = uniq_prod[\"keywords\"].astype(str).apply(correct_spelling)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z1-_GdxKHIjj"
      },
      "outputs": [],
      "source": [
        "# Create a DataFrame for indexing\n",
        "index_data = uniq_prod[['docno', 'corrected_keywords']]\n",
        "index_data.columns = ['docno', 'text']\n",
        "\n",
        "# Index the data\n",
        "indexer = pt.DFIndexer(\"./index\", overwrite=True)\n",
        "index_ref = indexer.index(index_data['text'], index_data['docno'])\n",
        "\n",
        "# Retrieve documents using BatchRetrieve\n",
        "retriever = pt.BatchRetrieve(index_ref, wmodel=\"BM25\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F8lxd8J_GCA7"
      },
      "outputs": [],
      "source": [
        "# Function to compute fuzzy relevance score\n",
        "def fuzzy_relevance(query, product_name, description, brand):\n",
        "    name_similarity = fuzz.token_set_ratio(query, product_name)\n",
        "    desc_similarity = fuzz.token_set_ratio(query, description)\n",
        "    brand_similarity = fuzz.token_set_ratio(query, brand)\n",
        "    partial_similarity = fuzz.partial_ratio(query, product_name + \" \" + description)\n",
        "    return 0.3 * name_similarity + 0.3 * brand_similarity + 0.2 * desc_similarity + 0.2 * partial_similarity\n",
        "    # return 0.5 * name_similarity + 0.3 * desc_similarity + 0.2 * partial_similarity\n",
        "\n",
        "\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "def infer_category(category):\n",
        "    category_synsets = wordnet.synsets(category)\n",
        "    hyponyms = set()\n",
        "\n",
        "    for synset in category_synsets:\n",
        "        for hyponym in synset.hyponyms():  # Get specific items under the category\n",
        "            hyponyms.update(hyponym.lemma_names())\n",
        "\n",
        "    return list(hyponyms)\n",
        "\n",
        "# print(get_hyponyms(\"furniture\"))  # Output: ['sofa', 'bed', 'chair', 'table', etc.]\n",
        "\n",
        "\n",
        "# Filter results by inferred category\n",
        "def filter_by_category(results, query):\n",
        "    category = infer_category(query)\n",
        "    if category:\n",
        "        results = results[results['product_category_tree'].str.contains(category, case=False)]\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S3ayB0PCPXNf"
      },
      "outputs": [],
      "source": [
        "# # Function to handle search and display results\n",
        "# def search_products_with_embeddings(query):\n",
        "#     # Step 1: Spell correction\n",
        "#     corrected_query = correct_spelling(query)\n",
        "#     print(f\"Corrected Query: {corrected_query}\")\n",
        "\n",
        "#     # Step 2: Expand query with related words\n",
        "#     querys = ' '.join(infer_category(query))\n",
        "#     expanded_query = query + \" \" + corrected_query + \" \" + expand_query(corrected_query)\n",
        "\n",
        "#     # Step 3: Retrieve documents using BM25\n",
        "#     results = retriever.search(expanded_query)\n",
        "#     results = results.merge(uniq_prod, on='docno', how='left')\n",
        "\n",
        "#     print(f\"Initial results count: {len(results)}\")  # Debugging check\n",
        "\n",
        "#     # Check if results are less than 5 and expand query further\n",
        "#     if len(results) < 5 and querys.strip():\n",
        "#         print(\"Expanding query further for better results...\")\n",
        "#         additional_expansion = expand_query(querys)\n",
        "\n",
        "#         if additional_expansion.strip():  # Ensure it's not empty\n",
        "#             expanded_query += \" \" + additional_expansion\n",
        "#             results = retriever.search(expanded_query)\n",
        "#             results = results.merge(uniq_prod, on='docno', how='left')\n",
        "\n",
        "#     print(f\"Expanded Query: {expanded_query}\")\n",
        "#     print(f\"Results count after expansion: {len(results)}\")  # Debugging check\n",
        "\n",
        "#     # Check if results are empty after BM25 retrieval\n",
        "#     if results.empty:\n",
        "#         print(\"No products found using BM25 retrieval.\")\n",
        "#         return suggest_alternatives(query)\n",
        "\n",
        "#     # Step 4: Filter by category\n",
        "#     results = filter_by_category(results, expanded_query)\n",
        "\n",
        "#     # Check if results are empty after category filtering\n",
        "#     if results.empty:\n",
        "#         print(\"No products found after category filtering.\")\n",
        "#         return suggest_alternatives(query)\n",
        "\n",
        "#     # Step 5: Compute semantic similarity\n",
        "#     try:\n",
        "#         query_embedding = model.encode([expanded_query])\n",
        "#         document_embeddings = model.encode(results['corrected_keywords'].tolist())\n",
        "\n",
        "#         if len(document_embeddings) == 0:\n",
        "#             print(\"No valid embeddings found for the retrieved documents.\")\n",
        "#             return suggest_alternatives(query)\n",
        "\n",
        "#         similarities = cosine_similarity(query_embedding, document_embeddings).flatten()\n",
        "#         results['similarity_score'] = similarities\n",
        "#     except Exception as e:\n",
        "#         print(f\"Error during semantic similarity computation: {e}\")\n",
        "#         return suggest_alternatives(query)\n",
        "\n",
        "#     # Step 6: Compute fuzzy relevance\n",
        "#     results['fuzzy_score'] = results.apply(\n",
        "#         lambda row: fuzzy_relevance(expanded_query, row['product_name'], row['description'], row['brand']), axis=1\n",
        "#     )\n",
        "\n",
        "#     # Step 7: Filter results based on semantic similarity threshold\n",
        "#     SEMANTIC_THRESHOLD = 0.6\n",
        "#     results = results[results['similarity_score'] >= SEMANTIC_THRESHOLD]\n",
        "\n",
        "#     if results.empty:\n",
        "#         print(\"No products found after semantic filtering.\")\n",
        "#         return suggest_alternatives(query)\n",
        "\n",
        "#     # Step 8: Rank results by combining BM25, semantic similarity, and fuzzy scores\n",
        "#     results['final_score'] = (\n",
        "#         results['score'] * 0.5 +\n",
        "#         results['similarity_score'] * 0.3 +\n",
        "#         results['fuzzy_score'] * 0.2\n",
        "#     )\n",
        "#     ranked_results = results.sort_values(by='final_score', ascending=False)\n",
        "\n",
        "#     if ranked_results.empty:\n",
        "#         print(\"Product not available.\")\n",
        "#         return suggest_alternatives(query)\n",
        "#     else:\n",
        "#         return ranked_results[['product_name', 'brand', 'description', 'final_score']].head(20)\n",
        "\n",
        "# # Suggest alternative products if no match is found\n",
        "# def suggest_alternatives(query):\n",
        "#     suggestions = retriever.search(query)\n",
        "#     if suggestions.empty:\n",
        "#         return \"No relevant products found.\"\n",
        "#     else:\n",
        "#         return suggestions.head(20)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qn0ylZXxk1wW"
      },
      "outputs": [],
      "source": [
        "# # Function to handle search and display results\n",
        "# def search_products_with_embeddings(query):\n",
        "#     # Step 1: Spell correction\n",
        "#     corrected_query = correct_spelling(query)\n",
        "#     print(f\"Corrected Query: {corrected_query}\")\n",
        "\n",
        "#     # Step 2: Expand query with related words\n",
        "#     # querys = ' '.join(infer_category(query))\n",
        "#     expanded_query = query + \" \" + corrected_query + \" \" + expand_query(corrected_query)\n",
        "\n",
        "#     # Step 3: Retrieve documents using BM25\n",
        "#     results = retriever.search(expanded_query)\n",
        "#     results = results.merge(uniq_prod, on='docno', how='left')\n",
        "#     pre_semantic_results = results.copy();\n",
        "\n",
        "#     print(f\"Initial results count: {len(results)}\")\n",
        "#     # print(results.head(10))\n",
        "\n",
        "#     # Step 4: Filter by category\n",
        "#     results = filter_by_category(results, expanded_query)\n",
        "#     print(f\"Results count after ctegory filter: {len(results)}\")\n",
        "\n",
        "#     # Step 5: Compute semantic similarity\n",
        "#     try:\n",
        "#         query_embedding = model.encode([expanded_query])\n",
        "#         document_embeddings = model.encode(results['corrected_keywords'].tolist())\n",
        "\n",
        "#         if len(document_embeddings) == 0:\n",
        "#             print(\"No valid embeddings found for the retrieved documents.\")\n",
        "#             return suggest_alternatives(query)\n",
        "\n",
        "#         similarities = cosine_similarity(query_embedding, document_embeddings).flatten()\n",
        "#         results['similarity_score'] = similarities\n",
        "#     except Exception as e:\n",
        "#         print(f\"Error during semantic similarity computation: {e}\")\n",
        "#         return suggest_alternatives(query)\n",
        "\n",
        "#     # Step 6: Compute fuzzy relevance\n",
        "#     results['fuzzy_score'] = results.apply(\n",
        "#         lambda row: fuzzy_relevance(expanded_query, row['product_name'], row['description'], row['brand']), axis=1\n",
        "#     )\n",
        "\n",
        "#     # Step 7: Filter results based on semantic similarity threshold\n",
        "#     SEMANTIC_THRESHOLD = 0.6\n",
        "#     results = results[results['similarity_score'] >= SEMANTIC_THRESHOLD]\n",
        "\n",
        "#     # Step 8: Rank results by combining BM25, semantic similarity, and fuzzy scores\n",
        "#     results['final_score'] = (\n",
        "#         results['score'] * 0.5 +\n",
        "#         results['similarity_score'] * 0.3 +\n",
        "#         results['fuzzy_score'] * 0.2\n",
        "#     )\n",
        "#     ranked_results = results.sort_values(by='final_score', ascending=False)\n",
        "\n",
        "#     print(f\"Final results count: {len(ranked_results)}\")\n",
        "\n",
        "\n",
        "#     #  If final results are still < 5, expand the query further and repeat search\n",
        "#     if len(ranked_results) < 5:\n",
        "#       querys = ' '.join(infer_category(query))\n",
        "#       if (querys.strip()):\n",
        "#         print(\"Expanding query further for better results...\")\n",
        "#         additional_expansion = expand_query(querys)\n",
        "\n",
        "#         if additional_expansion.strip():  # Ensure it's not empty\n",
        "#             expanded_query += \" \" + additional_expansion\n",
        "#             results = retriever.search(expanded_query)\n",
        "#             results = results.merge(uniq_prod, on='docno', how='left')\n",
        "\n",
        "#             # Repeat Steps 4-8 for updated results\n",
        "#             results = filter_by_category(results, expanded_query)\n",
        "\n",
        "#             try:\n",
        "#                 query_embedding = model.encode([expanded_query])\n",
        "#                 document_embeddings = model.encode(results['corrected_keywords'].tolist())\n",
        "\n",
        "#                 if len(document_embeddings) > 0:\n",
        "#                     similarities = cosine_similarity(query_embedding, document_embeddings).flatten()\n",
        "#                     results['similarity_score'] = similarities\n",
        "#             except Exception as e:\n",
        "#                 print(f\"Error during semantic similarity computation: {e}\")\n",
        "#                 return suggest_alternatives(query)\n",
        "\n",
        "#             results['fuzzy_score'] = results.apply(\n",
        "#                 lambda row: fuzzy_relevance(expanded_query, row['product_name'], row['description'], row['brand']), axis=1\n",
        "#             )\n",
        "\n",
        "#             results = results[results['similarity_score'] >= SEMANTIC_THRESHOLD]\n",
        "\n",
        "#             results['final_score'] = (\n",
        "#                 results['score'] * 0.5 +\n",
        "#                 results['similarity_score'] * 0.3 +\n",
        "#                 results['fuzzy_score'] * 0.2\n",
        "#             )\n",
        "#             ranked_results = results.sort_values(by='final_score', ascending=False)\n",
        "\n",
        "#         print(f\"Results count after additional expansion: {len(ranked_results)}\")\n",
        "\n",
        "#     # Display results or suggest alternatives\n",
        "#     if ranked_results.empty:\n",
        "#         print(\"Product not available.\")\n",
        "#         if results.empty:\n",
        "#           print(\"No products found after semantic filtering. Returning pre-semantic results.\")\n",
        "#           # return pre_semantic_results if not pre_semantic_results.empty else suggest_alternatives(query)\n",
        "#           return suggest_alternatives(query) if not retriever.search(query).empty else pre_semantic_results\n",
        "#         # return suggest_alternatives(query)\n",
        "#         # return suggest_alternatives(query) if pre_semantic_results.empty else pre_semantic_results\n",
        "#     else:\n",
        "#         return ranked_results[['product_name', 'brand', 'description', 'final_score']].head(20)\n",
        "\n",
        "# # Suggest alternative products if no match is found\n",
        "# def suggest_alternatives(query):\n",
        "#     suggestions = retriever.search(query)\n",
        "#     if suggestions.empty:\n",
        "#         return \"No relevant products found.\"\n",
        "#     else:\n",
        "#         # Merge suggestions with uniq_prod to get product details\n",
        "#         suggestions = suggestions.merge(uniq_prod, on='docno', how='left')\n",
        "#         return suggestions[['product_name', 'brand', 'description']].head(30)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IvDRr1bVxE3y"
      },
      "outputs": [],
      "source": [
        "# # Function to handle search and display results\n",
        "# def search_products_with_embeddings(query):\n",
        "#     # Step 1: Spell correction\n",
        "#     corrected_query = correct_spelling(query)\n",
        "#     print(f\"Corrected Query: {corrected_query}\")\n",
        "\n",
        "\n",
        "#     # Step 2: Expand query with related words\n",
        "#     expanded_query = f\"{query} {corrected_query} {expand_query(corrected_query)}\"\n",
        "#     print(expanded_query)\n",
        "\n",
        "\n",
        "#     # Step 3: Retrieve documents using BM25\n",
        "#     results = retriever.search(expanded_query)\n",
        "#     results = results.merge(uniq_prod, on=\"docno\", how=\"left\")\n",
        "#     pre_semantic_results = results.copy()\n",
        "\n",
        "\n",
        "#     # Step 4: Filter by category\n",
        "#     results = filter_by_category(results, expanded_query)\n",
        "#     print(f\"Results count after category filter: {len(results)}\")\n",
        "\n",
        "\n",
        "#     # Step 5: Compute semantic similarity\n",
        "#     try:\n",
        "#         query_embedding = model.encode([expanded_query])\n",
        "#         document_embeddings = model.encode(results[\"corrected_keywords\"].tolist())\n",
        "\n",
        "#         if not len(document_embeddings):\n",
        "#             print(\"No valid embeddings found for the retrieved documents.\")\n",
        "#             return suggest_alternatives(query, pre_semantic_results)\n",
        "\n",
        "#         results[\"similarity_score\"] = cosine_similarity(query_embedding, document_embeddings).flatten()\n",
        "#     except Exception as e:\n",
        "#         print(f\"Error during semantic similarity computation: {e}\")\n",
        "#         return suggest_alternatives(query, pre_semantic_results)\n",
        "\n",
        "\n",
        "\n",
        "#     # Step 6: Compute fuzzy relevance\n",
        "#     results[\"fuzzy_score\"] = results.apply(\n",
        "#         lambda row: fuzzy_relevance(expanded_query, row[\"product_name\"], row[\"description\"], row[\"brand\"]),\n",
        "#         axis=1,\n",
        "#     )\n",
        "\n",
        "\n",
        "\n",
        "#     # Step 7: Filter results based on semantic similarity threshold\n",
        "#     SEMANTIC_THRESHOLD = 0.6\n",
        "#     results = results[results[\"similarity_score\"] >= SEMANTIC_THRESHOLD]\n",
        "\n",
        "\n",
        "\n",
        "#     # Step 8: Rank results by combining BM25, semantic similarity, and fuzzy scores\n",
        "#     results[\"final_score\"] = (\n",
        "#         results[\"score\"] * 0.5 + results[\"similarity_score\"] * 0.3 + results[\"fuzzy_score\"] * 0.2\n",
        "#     )\n",
        "#     ranked_results = results.sort_values(by=\"final_score\", ascending=False)\n",
        "#     print(f\"Final results count: {len(ranked_results)}\")\n",
        "\n",
        "\n",
        "\n",
        "#     # Step 9 If final results are still < 5, expand the query further and repeat search\n",
        "#     if len(ranked_results) < 5:\n",
        "#         inferred_category = \" \".join(infer_category(query))\n",
        "#         if inferred_category.strip():\n",
        "#             print(\"Expanding query further for better results...\")\n",
        "#             additional_expansion = expand_query(inferred_category)\n",
        "\n",
        "#             if additional_expansion.strip():\n",
        "#                 expanded_query += f\" {additional_expansion}\"\n",
        "#                 results = retriever.search(expanded_query).merge(uniq_prod, on=\"docno\", how=\"left\")\n",
        "\n",
        "#                 # Repeat Steps 4-8 for updated results\n",
        "#                 results = filter_by_category(results, expanded_query)\n",
        "\n",
        "#                 try:\n",
        "#                     query_embedding = model.encode([expanded_query])\n",
        "#                     document_embeddings = model.encode(results[\"corrected_keywords\"].tolist())\n",
        "\n",
        "#                     if len(document_embeddings) > 0:\n",
        "#                         results[\"similarity_score\"] = cosine_similarity(query_embedding, document_embeddings).flatten()\n",
        "#                 except Exception as e:\n",
        "#                     print(f\"Error during semantic similarity computation: {e}\")\n",
        "#                     return suggest_alternatives(query, pre_semantic_results)\n",
        "\n",
        "#                 results[\"fuzzy_score\"] = results.apply(\n",
        "#                     lambda row: fuzzy_relevance(expanded_query, row[\"product_name\"], row[\"description\"], row[\"brand\"]),\n",
        "#                     axis=1,\n",
        "#                 )\n",
        "\n",
        "#                 results = results[results[\"similarity_score\"] >= SEMANTIC_THRESHOLD]\n",
        "\n",
        "#                 results[\"final_score\"] = (\n",
        "#                     results[\"score\"] * 0.5 + results[\"similarity_score\"] * 0.3 + results[\"fuzzy_score\"] * 0.2\n",
        "#                 )\n",
        "\n",
        "#                 ranked_results = pd.concat([ranked_results, results.sort_values(by=\"final_score\", ascending=False)])\n",
        "\n",
        "#             print(f\"Results count after additional expansion: {len(ranked_results)}\")\n",
        "\n",
        "\n",
        "#     # Display results or suggest alternatives\n",
        "#     if len(ranked_results)<5:\n",
        "#         if ranked_results.empty:\n",
        "#             print(\"Product not available.\")\n",
        "#             if results.empty:\n",
        "#                 print(\"No products found after semantic filtering. Returning pre-semantic results.\")\n",
        "#             print(ranked_results[[\"product_name\", \"brand\", \"description\", \"final_score\"]].head(20))\n",
        "#             return suggest_alternatives(query, pre_semantic_results)\n",
        "#         return suggest_alternatives(query, pre_semantic_results)\n",
        "\n",
        "#     # # return ranked_results[[\"product_name\", \"brand\", \"description\", \"final_score\"]].head(20)\n",
        "\n",
        "# # if len(ranked_results) < 5:\n",
        "# #     print(ranked_results[[\"product_name\", \"brand\", \"description\", \"final_score\"]].head(20))\n",
        "\n",
        "# #     if ranked_results.empty:\n",
        "# #         print(\"Product not available.\")\n",
        "\n",
        "# #         if results.empty:\n",
        "# #             print(\"No products found after semantic filtering. Returning pre-semantic results.\")\n",
        "\n",
        "# #         return suggest_alternatives(query, pre_semantic_results)\n",
        "\n",
        "# #     return suggest_alternatives(query, pre_semantic_results)\n",
        "\n",
        "\n",
        "\n",
        "# # Suggest alternative products if no match is found\n",
        "# def suggest_alternatives(query, pre_semantic_results):\n",
        "#     suggestions = retriever.search(query).merge(uniq_prod, on=\"docno\", how=\"left\")\n",
        "#     if suggestions.empty:\n",
        "#         return pre_semantic_results[[\"product_name\", \"brand\", \"description\"]].head(30)\n",
        "\n",
        "#     return suggestions[[\"product_name\", \"brand\", \"description\"]].head(30)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to handle search and display results\n",
        "def search_products_with_embeddings(query):\n",
        "    # Step 1: Spell correction\n",
        "    corrected_query = correct_spelling(query)\n",
        "    print(f\"Corrected Query: {corrected_query}\")\n",
        "\n",
        "    # Step 2: Expand query with related words\n",
        "    expanded_query = f\"{query} {corrected_query} {expand_query(corrected_query)}\"\n",
        "    print(expanded_query)\n",
        "\n",
        "    # Step 3: Retrieve documents using BM25\n",
        "    results = retriever.search(expanded_query).merge(uniq_prod, on=\"docno\", how=\"left\")\n",
        "    pre_semantic_results = results.copy()\n",
        "\n",
        "    # Step 4: Filter by category\n",
        "    results = filter_by_category(results, expanded_query)\n",
        "    print(f\"Results count after category filter: {len(results)}\")\n",
        "\n",
        "    # Step 5: Compute semantic similarity\n",
        "    try:\n",
        "        query_embedding = model.encode([expanded_query])\n",
        "        document_embeddings = model.encode(results[\"corrected_keywords\"].tolist())\n",
        "\n",
        "        if not len(document_embeddings):\n",
        "            print(\"No valid embeddings found for the retrieved documents.\")\n",
        "            return suggest_alternatives(query, pre_semantic_results)\n",
        "\n",
        "        results[\"similarity_score\"] = cosine_similarity(query_embedding, document_embeddings).flatten()\n",
        "    except Exception as e:\n",
        "        print(f\"Error during semantic similarity computation: {e}\")\n",
        "        return suggest_alternatives(query, pre_semantic_results)\n",
        "\n",
        "    # Step 6: Compute fuzzy relevance\n",
        "    results[\"fuzzy_score\"] = results.apply(\n",
        "        lambda row: fuzzy_relevance(expanded_query, row[\"product_name\"], row[\"description\"], row[\"brand\"]),\n",
        "        axis=1,\n",
        "    )\n",
        "\n",
        "    # Step 7: Filter results based on semantic similarity threshold\n",
        "    SEMANTIC_THRESHOLD = 0.6\n",
        "    results = results[results[\"similarity_score\"] >= SEMANTIC_THRESHOLD]\n",
        "\n",
        "    # Step 8: Rank results by combining BM25, semantic similarity, and fuzzy scores\n",
        "    results[\"final_score\"] = (\n",
        "        results[\"score\"] * 0.5 + results[\"similarity_score\"] * 0.3 + results[\"fuzzy_score\"] * 0.2\n",
        "    )\n",
        "    ranked_results = results.sort_values(by=\"final_score\", ascending=False)\n",
        "    print(f\"Final results count: {len(ranked_results)}\")\n",
        "\n",
        "    # Step 9: Expand query if results < 5\n",
        "    if len(ranked_results) < 5:\n",
        "        inferred_category = \" \".join(infer_category(query))\n",
        "        if inferred_category.strip():\n",
        "            print(\"Expanding query further for better results...\")\n",
        "            additional_expansion = expand_query(inferred_category)\n",
        "\n",
        "            if additional_expansion.strip():\n",
        "                expanded_query += f\" {additional_expansion}\"\n",
        "                results = retriever.search(expanded_query).merge(uniq_prod, on=\"docno\", how=\"left\")\n",
        "\n",
        "                # Repeat Steps 4-8 for updated results\n",
        "                results = filter_by_category(results, expanded_query)\n",
        "\n",
        "                try:\n",
        "                    query_embedding = model.encode([expanded_query])\n",
        "                    document_embeddings = model.encode(results[\"corrected_keywords\"].tolist())\n",
        "\n",
        "                    if len(document_embeddings) > 0:\n",
        "                        results[\"similarity_score\"] = cosine_similarity(query_embedding, document_embeddings).flatten()\n",
        "                except Exception as e:\n",
        "                    print(f\"Error during semantic similarity computation: {e}\")\n",
        "                    return suggest_alternatives(query, pre_semantic_results)\n",
        "\n",
        "                results[\"fuzzy_score\"] = results.apply(\n",
        "                    lambda row: fuzzy_relevance(expanded_query, row[\"product_name\"], row[\"description\"], row[\"brand\"]),\n",
        "                    axis=1,\n",
        "                )\n",
        "\n",
        "                results = results[results[\"similarity_score\"] >= SEMANTIC_THRESHOLD]\n",
        "\n",
        "                results[\"final_score\"] = (\n",
        "                    results[\"score\"] * 0.5 + results[\"similarity_score\"] * 0.3 + results[\"fuzzy_score\"] * 0.2\n",
        "                )\n",
        "\n",
        "                ranked_results = pd.concat([ranked_results, results.sort_values(by=\"final_score\", ascending=False)])\n",
        "\n",
        "            print(f\"Results count after additional expansion: {len(ranked_results)}\")\n",
        "\n",
        "    # Step 10: Display results or suggest alternatives\n",
        "    if len(ranked_results) > 5:\n",
        "      return ranked_results[[\"product_name\", \"brand\", \"description\", \"final_score\"]].head(20)\n",
        "    elif ranked_results.empty:\n",
        "        print(\"Product not available.\")\n",
        "        if results.empty:\n",
        "            print(\"No products found after semantic filtering. Returning pre-semantic results.\")\n",
        "        return suggest_alternatives(query, pre_semantic_results)\n",
        "    elif len(ranked_results) < 5:\n",
        "        res = pd.concat(ranked_results, suggest_alternatives(query, pre_semantic_results))\n",
        "        return res[[\"product_name\", \"brand\", \"description\", \"final_score\"]].head(20)\n",
        "\n",
        "\n",
        "# Suggest alternative products if no match is found\n",
        "def suggest_alternatives(query, pre_semantic_results):\n",
        "    suggestions = retriever.search(query).merge(uniq_prod, on=\"docno\", how=\"left\")\n",
        "    # if suggestions.empty:\n",
        "    if len(suggestions) < 10:\n",
        "        final_results = pd.concat([suggestions, pre_semantic_results])\n",
        "        if len(final_results) == 0:\n",
        "            return \"No Results Found\"\n",
        "        return final_results[[\"product_name\", \"brand\", \"description\"]].head(30)\n",
        "\n",
        "    return suggestions[[\"product_name\", \"brand\", \"description\"]].head(30)\n",
        "\n",
        "# def suggest_alternatives(query, pre_semantic_results):\n",
        "#     suggestions = retriever.search(query).merge(uniq_prod, on=\"docno\", how=\"left\")\n",
        "#     # if suggestions.empty:\n",
        "#     if len(suggestions) < 10:\n",
        "#         suggestions = suggestions if not suggestions.empty else pd.DataFrame(columns=pre_semantic_results.columns)\n",
        "#         final_results = pd.concat([suggestions, pre_semantic_results])\n",
        "#         return final_results[[\"product_name\", \"brand\", \"description\"]].head(30)\n",
        "#     else:\n",
        "#         return suggestions[[\"product_name\", \"brand\", \"description\"]].head(30) if not suggestions.empty else \"No Results Found\"\n"
      ],
      "metadata": {
        "id": "EnSCX5AubKK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JCqwemyJGjHx"
      },
      "outputs": [],
      "source": [
        "# Take user input\n",
        "user_query = input(\"Enter your search query: \")\n",
        "search_results = search_products_with_embeddings(user_query)\n",
        "print(\"\\nSearch Results:\")\n",
        "search_results"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AVVtmkJJbOqh"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}